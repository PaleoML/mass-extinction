{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "from scipy import interpolate\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import copy\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "colors = ['#88CCEE', '#882255', '#999933', '#332288', '#DDCC77', '#117733','#CC6677', '#44AA99', '#AA4499', '#DDDDDD', '#C70039', '#000000', '#D55E00', '#0072B2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and filter for minimum number of occurences\n",
    "# Returns a dict: extinction name -> pandas data frame\n",
    "\n",
    "min_occurrence = 3\n",
    "\n",
    "# Cleaned data - three extinctions\n",
    "data_base = Path('data_20ext')\n",
    "\n",
    "# List of characteristics\n",
    "# It's important to place non-categorical characteristics at the END of this list.\n",
    "characteristics = ['tiering', 'motility', 'feeding', 'protein', 'reproduction', 'mineralogy', 'physiology', 'geoplates', 'species']\n",
    "categorical_chars = ['tiering', 'motility', 'feeding', 'protein', 'reproduction', 'mineralogy', 'physiology']\n",
    "\n",
    "# Loading\n",
    "events = [x for x in data_base.iterdir() if x.is_file() and x.suffix == '.csv']\n",
    "datasets = {x.name[:x.name.find('_')]: pd.read_csv(x, encoding=\"ISO-8859-1\") for x in events}\n",
    "\n",
    "# Filtering for number of occurrences, species and geoplates\n",
    "datasets = {key: val[val['t_occurrences'] >= min_occurrence] for key, val in datasets.items()}\n",
    "datasets = {key: val[val['species'] > 0] for key, val in datasets.items()}\n",
    "datasets = {key: val[val['geoplates'] > 0] for key, val in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ext in datasets.keys():\n",
    "    datasets[ext].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of the three major extinctions\n",
    "ext_order = sorted(list(datasets.keys()))\n",
    "ext_name = {'Cha': 'P/Tr', \n",
    "            'Rhe': 'Tr/J',\n",
    "            'Maa': 'K/Pg',\n",
    "            'Cen': 'OAE2',\n",
    "            'Tha': 'PETM'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homogenize spelling (capitalization)\n",
    "for dataset in datasets.values():\n",
    "    dataset.loc[dataset[\"mineralogy\"] == \"High Mg calcite\", \"mineralogy\"] = \"high Mg calcite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resamples the data sets to split entries with multiple values into multiple rows with lower weight\n",
    "\n",
    "def traverse(row, chars, params, weight):\n",
    "    # Goes over the row recursively and splits if it finds multiple values for one characteristic\n",
    "    if len(chars) == 0:\n",
    "        params['weight'] = weight\n",
    "        return [params]\n",
    "    else:\n",
    "        c = chars[0]\n",
    "        v = row[c]\n",
    "        if isinstance(v, int) or isinstance(v, float):\n",
    "            if c not in categorical_chars or v < 10:\n",
    "                params[c] = v\n",
    "                return traverse(row, chars[1:], params, weight)\n",
    "            else:\n",
    "                res = []\n",
    "                params1 = copy.deepcopy(params)\n",
    "                params1[c] = v // 10\n",
    "                res += traverse(row, chars[1:], params1, weight / 2)\n",
    "                params2 = copy.deepcopy(params)\n",
    "                params2[c] = v % 10\n",
    "                res += traverse(row, chars[1:], params2, weight / 2)\n",
    "                return res\n",
    "        else:\n",
    "            res = []\n",
    "            parts = v.split('/')\n",
    "                \n",
    "            for subv in parts:\n",
    "                subv = subv.strip()\n",
    "                subparams = copy.deepcopy(params)\n",
    "                subparams[c] = subv\n",
    "                res += traverse(row, chars[1:], subparams, weight / len(parts))\n",
    "            return res\n",
    "\n",
    "def rewrite_multi_category(dataset):\n",
    "    # Iterates over the rows for the traversal\n",
    "    res = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        start_cols = [x for x in dataset.columns if x not in characteristics]\n",
    "        params = {x: row[x] for x in start_cols}\n",
    "        \n",
    "        res += traverse(row, characteristics, params, 1)\n",
    "    \n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "def repeat_rows(dataset):\n",
    "    # Repeat rows according to the inverse of their weights\n",
    "    wmin = np.min(dataset['weight'])\n",
    "    res = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        res += int(np.round(row['weight'] / wmin)) * [row]\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "# Run the actual resampling\n",
    "datasets = {k: rewrite_multi_category(v) for k, v in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {}\n",
    "\n",
    "# Fixed orders\n",
    "orders[\"tiering\"] = [\"pelagic\", \"erect\", \"epifaunal\", \"semi-infaunal\", \"shallow infaunal\", \"deep infaunal\"]\n",
    "orders[\"motility\"] = [\"fast, motile\", \"slow, motile\", \"facultative, unattached\", \"facultative, attached\", \"stationary, unattached\", \"stationary, attached\"]\n",
    "orders[\"feeding\"] = [\"suspension\", \"surface deposit\", \"miner\", \"grazer\", \"predator\", \"symbionts\"]\n",
    "orders[\"protein\"] = ['hemerythrin', 'hemocyanin', 'hemoglobin', 'other']\n",
    "orders[\"reproduction\"] = ['non-broadcaster', 'intermediate', 'broadcaster']\n",
    "orders[\"mineralogy\"] = ['aragonite', 'high Mg calcite', 'low Mg calcite', 'bimineralic', 'phosphatic', 'chitin', 'gorgonin', 'silica', 'soft-bodied']\n",
    "orders[\"physiology\"] = ['heavy carbonate load', 'moderate carbonate load', 'little or no carbonate load']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extinctions = ext_order\n",
    "n_ext = len(extinctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(data):\n",
    "    # Returns numpy matrices of features and labels\n",
    "    return data[characteristics].values, data['extinct'].values\n",
    "\n",
    "def prepare_data(dataset, split=True):\n",
    "    if split:\n",
    "        # Splits data into training and test with a ratio of 80 : 20.\n",
    "        # Ensures that all samples from the same base index are in the same split to avoid knowledge leaks.\n",
    "        # Multiple occurrence of the same sample is a result from the resampling.\n",
    "        base_idx = np.unique(dataset.index)\n",
    "        \n",
    "        splits = np.arange(5).repeat(len(base_idx) // 5 + 1)\n",
    "        np.random.shuffle(splits)\n",
    "        splits = splits[:len(base_idx)]\n",
    "        test_idx = base_idx[splits == 0]\n",
    "\n",
    "        train_data = dataset[~dataset.index.isin(test_idx)]\n",
    "        test_data = dataset[dataset.index.isin(test_idx)]\n",
    "\n",
    "        x_train, y_train = split_xy(train_data)\n",
    "        x_test, y_test = split_xy(test_data)\n",
    "    \n",
    "        return x_train, y_train, x_test, y_test\n",
    "        \n",
    "    else:\n",
    "        return split_xy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a catboost model\n",
    "def train_model(x_train, y_train, x_test, y_test, random_seed=0, **kwargs):\n",
    "    model = CatBoostClassifier(loss_function=\"Logloss\", random_seed=random_seed)\n",
    "\n",
    "    # Sets all features a categorical except e_occurrences\n",
    "    model.fit(x_train, y_train, np.arange(len(categorical_chars)), eval_set=(x_test, y_test), **kwargs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(iterations=10):\n",
    "    # Trains models and evaluates feature importance and performance on the different data sets for each iteration\n",
    "    # The results are stored in the variables res_full (AUC scores) and feat_imp (feature importances)\n",
    "\n",
    "    res_full = np.zeros((iterations, n_ext, n_ext))\n",
    "    res_full_train = np.zeros((iterations, n_ext))\n",
    "    feat_imp = np.zeros((iterations, n_ext, len(characteristics)))\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    \n",
    "    data = None\n",
    "    model = None\n",
    "    \n",
    "    for it in tqdm(range(iterations)):\n",
    "        res = res_full[it]\n",
    "        res_train = res_full_train[it]\n",
    "        \n",
    "        for i, base in enumerate(extinctions):\n",
    "            data = prepare_data(datasets[base])\n",
    "                        \n",
    "            model = train_model(*data, random_seed=it, plot=False, verbose=False, use_best_model=False)\n",
    "\n",
    "            feat_imp[it, i] = model.feature_importances_\n",
    "\n",
    "            x_train, y_train, x_test, y_test = data\n",
    "\n",
    "            train_auc = metrics.roc_auc_score(y_train, model.predict_log_proba(x_train)[:, 1])\n",
    "            test_auc = metrics.roc_auc_score(y_test, model.predict_log_proba(x_test)[:, 1])\n",
    "            res_train[i] = train_auc\n",
    "            res[i, i] = test_auc\n",
    "            \n",
    "            fpr[it, base, base], tpr[it, base, base], _ = metrics.roc_curve(\n",
    "                y_test, model.predict_log_proba(x_test)[:, 1])\n",
    "\n",
    "            for j, ext in enumerate(extinctions):\n",
    "                if ext == base:\n",
    "                    continue\n",
    "                dataset = datasets[ext]\n",
    "\n",
    "                x, y = split_xy(dataset)\n",
    "\n",
    "                res[i, j] = metrics.roc_auc_score(y, model.predict_log_proba(x)[:, 1])\n",
    "                fpr[it, base, ext], tpr[it, base, ext], _ = metrics.roc_curve(\n",
    "                    y, model.predict_log_proba(x)[:, 1])\n",
    "    \n",
    "    return res_full, feat_imp, fpr, tpr, res_full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                | 2/10 [18:26<1:13:51, 553.88s/it]"
     ]
    }
   ],
   "source": [
    "res_full_noocc, feat_imp_noocc, fpr_noocc, tpr_noocc, res_train = run_experiments(iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "x = feat_imp_noocc.reshape((-1,) + feat_imp_noocc.shape[2:])\n",
    "\n",
    "pca.fit(x)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i, ext in enumerate(extinctions):\n",
    "    trans = pca.transform(feat_imp_noocc[:, i])\n",
    "    \n",
    "    marker = 'o'\n",
    "    ms = 8\n",
    "    z = 4\n",
    "    lw = 2\n",
    "    mew = 0\n",
    "    mec = 'k'\n",
    "    \n",
    "    if ext[:3] in [\"Cha\", \"Rhe\", \"Maa\"]:\n",
    "        marker = '*'\n",
    "        ms=25\n",
    "        c = f\"C3\"\n",
    "        z = 10\n",
    "        lw = 4\n",
    "        mew = 1.5\n",
    "    elif ext[:3] in [\"Cen\", \"Tha\"]:\n",
    "        c = \"C0\"\n",
    "        z = 5\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        c = \"grey\"\n",
    "        \n",
    "    x_mean = np.mean(trans[:, 0])\n",
    "    y_mean = np.mean(trans[:, 1])\n",
    "    x_std = np.std(trans[:, 0])\n",
    "    y_std = np.std(trans[:, 1])\n",
    "    \n",
    "    ax.plot([x_mean - x_std, x_mean + x_std], [y_mean, y_mean], '-', c=c, lw=lw, zorder=z)\n",
    "    ax.plot([x_mean, x_mean], [y_mean - y_std, y_mean + y_std], '-', c=c, lw=lw, zorder=z)\n",
    "    ax.plot(x_mean, y_mean, ms=ms, linewidth=0.4, marker=marker, label=ext[:-3], c=c, zorder=z, mew=mew, mec=mec)\n",
    "    \n",
    "    if c != \"grey\":\n",
    "        ax.text(x_mean + 0.3, y_mean + 0.3 , ext_name[ext[:3]], c=c, weight=\"bold\")\n",
    "    \n",
    "var = pca.explained_variance_ratio_\n",
    "ax.set_xlabel(f\"Component 1 [{var[0]:.2f}]\")\n",
    "ax.set_ylabel(f\"Component 2 [{var[1]:.2f}]\")\n",
    "\n",
    "root_x = 0\n",
    "root_y = 0\n",
    "length = 6.5\n",
    "\n",
    "for i, char in enumerate(characteristics):\n",
    "    wx = pca.components_[0, i] * length\n",
    "    wy = pca.components_[1, i] * length\n",
    "    \n",
    "    if wx ** 2 + wy ** 2 > (length / 3) ** 2:\n",
    "        # Only plot sufficiently relevant components\n",
    "        ax.arrow(root_x, root_y, wx, wy, color='k', width=0.05, head_width=0.5, length_includes_head=True)\n",
    "        ax.text(root_x + wx + 0.2, root_y + wy + 0.2, char.capitalize(), c='k')\n",
    "    else:\n",
    "        ax.arrow(root_x, root_y, wx, wy, color='k', width=0.02, head_width=0.2, length_includes_head=True)\n",
    "    \n",
    "fig.savefig(\"images/pca.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
